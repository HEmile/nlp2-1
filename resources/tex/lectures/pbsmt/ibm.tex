\section{Word-based SMT}
\subsection{IBM models 1 and 2}
\frame{
    \frametitle{The Noisy-Channel approach}
	
	Bayes rule 
	
	$$P(E|F) = \frac{P(E)P(F|E)}{P(F)}$$
	
	Inference
	
	$$\hat{E} = \argmax_E P(E)P(F|E)$$
	
	Estimation
	
	\begin{itemize}
		\item $P(E)$ $n$-gram LM
		\item $P(F|E)$ ...
	\end{itemize}
	
}

\frame{
    \frametitle{The IBM models}

		$$P(F|E) = \sum_A P(A,F|E) $$
	\begin{center}

    	 \includegraphics[width=0.8\textwidth]{"img/wilker-align"}
	\end{center}
}


\frame{
    \frametitle{Models 1 and 2}
	\begin{align*}
	P(F, A | E) &= P(m| E) \prod_{j=1}^m P(a_j,f_j | a_1^{j-1}, f^{j-1}_1,m, E) \\
		&= P(m| E) \prod_{j=1}^m P(a_j|a_1^{j-1}, f_1^{j-1}, m, E) P (f_j | a_1^{j}, f^{j-1}_1,m, E) 
	\end{align*}
	\pause
	\begin{itemize}
	\item lexical translation 
		$P(f_j | a_1^{j}, f^{j-1}_1,m, E) = t(f_j|e_i)$
	\item alignment $P(a_j| \dots)$
		\begin{itemize}
		\item IBM1: $\sim unif(l+1)$
		\item IBM2: $= a(i|j,m,l)$
		\item HMM: $= a(i|a_{j-1},l)$
		\end{itemize}
	\end{itemize}
}

\frame{
    \frametitle{Decoding with models 1 \& 2?}
	\begin{center}
    	 \includegraphics[width=0.8\textwidth]{"img/wilker-align"}
	\end{center}
	
	~
	\pause
	how to explain insertions on the English side?
}

\subsection{Model 3}
\frame{
    \frametitle{Modelling word fertility}

	\begin{itemize}
	\item {\em fertility}: number of words generated by an English words
	\item Generative story
	\begin{itemize}
	\item choose fertility for $e_i$
	\item choose French words generated for each $e_i$
	\item reorder French words
	\end{itemize}
	\pause
	\item parameters: fertility, translation, distortion, null-word
	\pause
	\item inference is intractable: \\
	~ E step in neighbourhood of Viterbi alignment
	\end{itemize}
}

\frame{
    \frametitle{Generative story}
	\begin{center}
       \only<1>{
                \includegraphics[width=0.8\textwidth]{"img/ibm3"}
        }
        \only<2>{
                \includegraphics[width=0.8\textwidth]{"img/ibm3-1"}
        }
        \only<3>{
                \includegraphics[width=0.8\textwidth]{"img/ibm3-2"}
        }
        \only<4>{
                \includegraphics[width=0.8\textwidth]{"img/ibm3-3"}
        }
        \only<5>{
                \includegraphics[width=0.8\textwidth]{"img/ibm3-4"}
        }
	\end{center}

}

\frame{
    \frametitle{Conclusion}
		
	\begin{center}
       \includegraphics[width=0.5\textwidth]{"img/wilker-ibm-table"}
	\end{center}

	~

	\begin{itemize}
	\item IBM models 1 and 2 are too weak  for decoding 
	\item decoding is NP-complete (for phrase-based models too) 
	\item asymmetry is unsatisfactory from linguistic perspective
	\end{itemize}
	
	
}

