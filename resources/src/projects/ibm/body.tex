
This project will help you familiarise yourself with word-based
models.  Word-based models remain at the core of today's SMT systems
in the form of alignment models.
You will implement the simplest (though still widely used) word-based
models, namely, IBM model 1, a lexical translation model, together with
some improvements to the difficulties mentioned in \cite{Moore:2004:IBM1}, 
and IBM model 2, which models an impoverished form of word alignments.

In summary, your task is to

\begin{itemize}
	\item implement IBM model 1
	\item implement IBM model 2
	\item try to think up and implement improvements to the problems 
              of IBM Model 1 as discussed in \cite{Moore:2004:IBM1}, or borrow from that paper,
%  \item (Optional: come up with and implement your own further improvements of the alignment model)
	\item write a report where you compare the baseline models and the
    improvements. Your report should also present learning curves
    where applicable along with a discussion explaining aspects such
    as non-convexity, stability and convergence.
    \item the structure of the report and what is expected to be submitted is made available in the
      separate instruction made available to you by the TAs.
\end{itemize}

\section{IBM model 1}

\begin{enumerate}
	\item implement IBM model 1 and its EM training \citep{Brown+1993:smt};
	\item plot the evolution of the log likelihood function as a function of the iteration;
	\item obtain Viterbi alignments for every sentence pair in a test corpus and compute 
              AER using a gold-standard provided by the assistant;
\end{enumerate}

In your report, you should also consider the limitations of Model 1
as described by \cite{Moore:2004:IBM1}, and find examples in your
output to illustrate these limitations.

\section{IBM model 2}

\begin{enumerate}
  \item extend your previous model by implementing a full IBM model 2 \citep{Brown+1993:smt};
  \item IBM 2 is non-convex, thus you will see that optimising the log-likelihood function is 
        not as trivial as in the case of IBM model 1, particularly, convergence will depend 
        on how you initialise the model parameters, you will try
  \begin{itemize}
    \item uniform initialisation
    \item random initialisation (try 3 different starting points)
    \item initialise the lexical parameters using the output of a complete run of model 1
  \end{itemize}
  \item plot the log-likelihood function as a function of the iteration for all these cases
  \item compare to IBM model 1 in terms of AER in the test set
\end{enumerate}

\section{Data}
The training and test data is taken from the Canadian Hansards (parliament proceedings). The data consists
of preprocessed sentence pairs (please do not further pre-process the data). There are two files, on for the
English and one for the French sentences. Sentences with the same line number are translations of each other.
The training data is available \href{https://github.com/wilkeraziz/uva-nlp2/blob/gh-pages/resources/project1/NLP2_Project1_data.tar.gz}{here}. The test data and an evaluation script will be made available in due time.

\section{Improvements to IBM model 1 from \cite{Moore:2004:IBM1}}

\citet{Moore:2004:IBM1} proposes to improve IBM model 1 by
\begin{enumerate} 
  \item smoothing counts for rare words
  \item assigning more probability mass to \textsc{Null} alignments
  \item heuristic initialisation of lexical parameters
\end{enumerate}

You should implement {\bf two} out of these three improvements and run your evaluations
to measure the extent of their effect.
If you compare the example sentences from the original model 1 to the
alignments from the extended version, have they improved?
%
%
\section{Further extensions not included in Project I}
These extensions are actually not part of the Project 1 but they provide you with
some material and questions to dwell on related to word alignment models.

In case you really are interested and are keen on implementing some of these
extensions, showing that you've also read some of this extra literature, 
you could earn yourself extra bonus points on top of Project 1.
%
%
\subsection*{Smoothing}

The add-$n$ smoothing used by \cite{Moore:2004:IBM1} is relatively
crude, and potentially a more sophisticated method might improve
alignments.  For instance, \citet{Riley+2012:VBWA} address the problem
of sparse distributions for rare words by smoothing expected counts in
the M-step using a technique called \emph{variational Bayes}.


\subsection*{Symmetric alignments}

IBM models 1 and 2 are asymmetric models, \citep{Liang+2006:ABA} introduce a simple yet effective way to make the parameters of source-to-target and target-to-source alignments agree effectively learning a symmetric model.

\subsection*{Over-parameterisation}
IBM model 2 is simple and inference is efficient, which is particularly important given that parallel corpora might contain millions of sentences. 
However, estimation for IBM model 2 is somewhat crude due its over-parameterisation. 
The model explicitly conditions on sentence length (source and target) and on alignment point amounting to $O(n^2)$ parameters.
\cite{Dyer+2013:IBM2} use a parametric alignment distribution which reduces the number of parameters to $2$ (a null alignment probability and a precision parameter).

\subsection*{Non-convexity}

The log-likelihood function under IBM model 2 is non-convex, thus unlike the case of IBM model 1, EM is not guaranteed to converge to a global optimum.
That complicates parameter estimation as EM becomes sensitive to initialisation.
\cite{Simion+2013:IBM2} relax IBM model 2 obtaining a closely related model for which the log-likelihood objective is convex. %Parameter estimation is done with a gradient-based optimisation method.

\subsection*{Semantic similarity}


The lexical parameters of IBM model 1 are unaware to the intuition that the semantic relationship between words should in principle help obtain better translation models.
\cite{Songyot+2014:WAWS} learn word similarities and integrate those with IBM model 1 by marginalising over similar words.
The authors choose to learn a probability distribution over similar words using a neural network.
Interacting with a neural network toolkit is beyond scope, so for this task you may choose to use a simpler framework. 
For instance, you can employ ideas from distributed semantics without necessarily training a neural network.\footnote{Or the assistant could provide students with probability distributions $p(f'|f)$ and $p(e'|e)$ (obtained for instance from PPDB).}

\subsection*{Inference}

Inference for IBM models is typically done with the Viterbi algorithm, that is, one typically produces the alignment which has highest probability under the model given the sentence pair.
\citet{Kumar+2002:MBRWA} propose to use a minimum risk criterion. 
In minimum Bayes risk (MBR) decoding, one chooses the hypothesis (set of alignment points) which minimises the expected loss or {\bf risk}.
A loss measures how compatible a prediction is with a true response.
If you choose this extension, you will implement MBR decoding and experiment with 3 kinds of loss functions: i) a loss inspired by alignment error rate (AER), ii) a loss defined in terms of automatic word-cluster distance, and iii) a loss defined in terms of paraphrases. 
