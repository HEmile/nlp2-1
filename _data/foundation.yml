-
  layout: lecture
  selected: y
  date: 2016-03-29
  img: introduction
  title: Introduction
  instructor: "Khalil Sima'an"
  note: 
  abstract: >
    In this introductory lecture we will discuss the role of machine translation in society, 
    the nature of translation data, and we will also present an overview of the history of MT. 
  background:
    - "For you to get a sense of the task, learn more about key concepts such as words, sentence, parallel corpora (the most important translation data), read chapters 1-2 of [Koehn's 2010 book](http://www.statmt.org/book/)."
  discussion:
  slides: resources/slides/introduction.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-03-31
  img: alignments
  title: Word alignment models
  instructor: Philip Schulz
  note: 
  abstract: >
    In this session we will discuss the IBM word alignment models 1 and 2. 
    We will view them as constrained mixture models. After motivating the models intuitively, we will develop them formally and devise an EM parameter estimation algorithm for them. 
    We will also try to understand why the resulting optimisation problem contains multiple optima. 
  background:
    - "Philip Schulz has made available a nice [tutorial](resources/papers/Schulz-IBM12-Tutorial.pdf) which is the basis for his lecture."
    - "In order to get a more concrete view of implementation, including pseudo-codes for the EM procedure, read M. Collins's [lecture notes](resources/papers/CollinsIBM.pdf)."
  further: 
    - "[R. Moore (2004)](http://www.aclweb.org/anthology/P/P04/P04-1066.pdf) discusses a number of problems with IBM model 1 involving 1) null words, 2) rare words, and 3) initialisation for EM. If you can understand the problems he raises and the solutions he proposes, you are really getting it about IBM1. Also, implementing some of his improvements will be worth some extra points in project 1."
    - "[Toutanova et al. (2011)](http://www.aclweb.org/anthology/P11-2081) discuss the non-strict convexity of IBM1. This is related to the non-identifiability problem discussed in class."
    - "The original [IBM paper](http://www.aclweb.org/anthology/J93-2003) is very dense, but if you want to go around teaching IBM models you should probably read it a few times."
    - "[Vogel et al. 1996](http://www.aclweb.org/anthology/C96-2141) revisit the assumption that alignments links are independent."
    - "[Dyer et al. 2013](http://www.aclweb.org/anthology/N13-1073.pdf) revisit the parameterisation of IBM model 2. Their model, called 'fast_align' achieves state-of-the-art performance and a robust and efficient [implementation](https://github.com/clab/fast_align) is available."
  discussion:
  slides:
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-04-05
  img: itg
  title: Inversion Transduction Grammars
  instructor: Wilker Aziz
  note: 
  abstract: In this lecture we will present alignments models that impose explicit reordering constraints through a grammar. We will cover bitext parsing, inference and parameter estimation.
  background:
    - "ITG is a form of context-free transducer. Thus the more you know about context-free grammars (CFGs) and their probabilistic extensions (PCFGs), the better. You can start with M Collins's [lecture notes](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf) on PCFGs. In estimating parameters for a PCFG with EM we need expectations (as always), there is an efficient algorithm to compute those from parse forests. Again, M Collins's has nice [lecture notes](http://www.cs.columbia.edu/~mcollins/io.pdf) on this algorithm (the Inside-Outside algorithm) as well as EM for PCFGs."
    - "I will discuss bitext parsing using deductive systems, if you are not familiar with that you might want to read a good [tutorial](http://arxiv.org/abs/cmp-lg/9404008)."  
    - "[Wu (1997)](https://www.aclweb.org/anthology/J/J97/J97-3002.pdf) is the best introduction to ITG in the literature."
    - "IBM alignment models fit within a larger translation component inspired by the noisy channel approach from information theory. ITGs can also play that role, see [Wu and Wong (1998)](http://www.aclweb.org/anthology/P98-2230). "
  further:
    - "[Dyer (2010)](http://www.aclweb.org/anthology/N10-1033) shows that a cascade of monolingual parsers is a very neat and general way to perform bitext parsing."
    - "ITGs have a very generic vocabulary of nonterminals leading to very flat distributions. One can use the idea of state splitting to refine the vocabulary of a vanilla ITG. This is very difficul to do in terms of MLE. [Blunsom et al. (2008)](http://papers.nips.cc/paper/3453-bayesian-synchronous-grammar-induction) develop a non-parametric Bayesian model which learns how far to refine the vocabulary of a vanilla ITG. [Blunsom and Cohn (2010)](http://www.aclweb.org/anthology/N10-1028) build on that work and propose a more efficient inference technique based on slice sampling."
    - "You probably want to wait until after the lecture on 'Phrase-based models' before you read the following papers."
    - "Phrase-based ITGs have never shown state-of-the-art performance because of its degenerate maximum likelihood estimates. [Mylonakis and Sima'an (2010)](http://www.aclweb.org/anthology/W10-2915) propose a better estimation procedure."
    - "[Neubig et al. (2011)](http://www.aclweb.org/anthology/P11-1064) apply non-parametric Bayesian methods to avoid degenerate maximum likelihood estimates for phrase-based ITGs."
    - "ITGs impose hard constraints on reordering, yet these constraints are not as unmotivated as an ad-hoc distortion limit. [Zens et al. (2004)](http://www.aclweb.org/anthology/C04-1030) investigate the effectiveness of ITG-style constraints in a phrase-based decoder. They also compare it to IBM-style constraints (namely, distortion limit)." 
  discussion:
  slides: resources/slides/itg.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-04-07
  img: pbsmt
  title: Phrase-based models
  instructor: Sophie Arnoult
  note: 
  abstract: |
    In this lecture we will introduce phrase-based models. 
    We will motivate the need for larger translation units than word pairs, and show how their modelling introduces a new issue: phrase segmentation. 
    We will see how standard phrase-based SMT deals with this issue, and present estimation and inference under this model.
  background:
    - "Koehn et al. 2003. [Statistial Phrase-Based Translation](http://www.aclweb.org/anthology/N/N03/N03-1017.pdf)"
    - "[Kumar and Byrne (2003)](http://www.aclweb.org/anthology/N03-1019) present a derivation and implementation of the [alignment template model](http://www.aclweb.org/anthology/W99-0604) using FSTs."
  further:
    - "On estimation:"
    - "[Marcu and Wong (2002)](http://www.aclweb.org/anthology/W02-1018) proposed a model that is conceptually very similar to the IBM models 1 and 2, but applied to phrases instead of words. Estimation in that model is intractable, requiring initial approximations and hard EM."
    - "[DeNero et al. (2006)](http://www.aclweb.org/anthology/W06-3105) inferred phrase translation estimates in a (uniform) segmentation space constrained by word alignments trained with the IBM models. This leads to degenerate estimates."
    - "[Mylonakis and Sima'an (2008)](http://www.aclweb.org/anthology/D08-1066) used word alignments and the phrase-extraction method of [Koehn et al. (2003)](http://www.aclweb.org/anthology/N/N03/N03-1017.pdf), but infer phrase-translation estimates with EM. They restrict the segmentation space to ITG derivations, and use cross-validation techniques to avoid overfitting."
    - "[DeNero and Klein (2008)](http://www.aclweb.org/anthology/P08-2007) showed that estimation for phrase-based models is NP-complete." 
    - "On inference:"
    - "[Sima'an (2002)](https://staff.fnwi.uva.nl/k.simaan/D-Papers/Grammars01.pdf) proved that disambiguation of formal grammars (Stochastic Context-Free and Tree-Substitution Grammars) is NP-complete. The result carries over to any problem where one needs to sum over derivations (or segmentations in the case of this lecture)."  
    - "[Knight (1999)](http://www.aclweb.org/anthology/J/J99/J99-4005.pdf) proved that decoding (for word-based models) is NP-complete, and laid out the analogy between decoding and the Travelling Salesman Problem (TSP)."
    - "[Zaslavskiy et al. (2009)](http://www.aclweb.org/anthology/P/P09/P09-1038.pdf) proposed to apply TSP solvers for decoding instead of beam-search decoding."
    - "[Aziz et al. (2014)](http://www.aclweb.org/anthology/D14-1131) replace beam-search decoding by an exact coarse-to-fine search inspired by rejection sampling."
  slides: resources/slides/pbsmt.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-04-12
  img: hpbsmt
  title: Hierarchical models
  instructor: Wilker Aziz
  note: 
  abstract: In this lecture we will generalise the phrase-based approach to include phrases with gaps.
  background:
    - "David Chiang. [An introduction to synchronous grammars](resources/papers/ChiangSCFG.pdf)"
    - "Chiang. 2007. [Hierarchical Phrase-Based Translation](http://www.mitpressjournals.org/doi/pdf/10.1162/coli.2007.33.2.201)"
    - "Chiang. 2005. [A Hierarchical Phrase-Based Model for Statistical Machine Translation](http://www.aclweb.org/anthology/P05-1033)" 
  discussion:
  slides: resources/slides/hpbsmt.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-04-14
  img: tuning
  title: Evaluation and Tuning
  instructor: "Miloš Stanojević"
  note: 
  abstract: >
    In this lecture we will talk about automatic evaluation and tuning. 
    Evaluation in Machine Translation is dealing with automatic assesment of the translation quality. 
    Unlike many machine learning tasks, here there is no unique gold truth to which we can compare the translation -- there are many equally good translations. 
    Also translations can be good or bad in many dimensions: morphology, syntax or semantics. 
    Evaluation is important not only for assesing the quality of the final system but also for automatic tuning of the systems. 
    In the second part of the lecture we will cover few algorithms that allow us to fine tune many parameters that our MT system can have (weights for language model, translation model, word penalty...).
  background:
      - "A good survey of evaluation methods can be found in Chapter 8 of [Koehn's book](http://www.statmt.org/book/). You can also refer to statmt's survey [page](http://www.statmt.org/survey/Topic/Evaluation)"
      - "A [survey](http://www.phontron.com/paper/neubig16cl.pdf) of optimisation techniques for machine translation."
      - "MERT is nicely explained in [Koehn's book](http://www.statmt.org/book/) (Chapter 9.3.2: Powell search algorithm)"
      - "MT tuning can be seen as a learning to rank task. In learning to rank, a typical approach is to approximate a complex ranking task by a series of pairwise ranking decision. This idea has been popularised in MT by the algorithm [PRO](http://www.aclweb.org/anthology/D11-1125)."
      - "For a nice overview of large margin discriminative learning in MT refer to [(Eidelman, 2012)](http://www.aclweb.org/anthology/W12-3160)"
  further:
      - "Read [(Lopez, 2012)](http://www.aclweb.org/anthology/W/W12/W12-3101.pdf) for a formal view of evaluation of automatic evaluation metrics (you read it right!)." 
      - "For MIRA with batched updates in MT refer to [(Cherry and Forster, 2012)](http://www.aclweb.org/anthology/N12-1047)"
  discussion:
  slides: resources/slides/eval-tuning.pdf
  code: 
  data: 
