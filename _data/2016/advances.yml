-
  layout: lecture
  selected: y
  date: 2016-04-21
  img: rg
  uid: rg
  title: Word-order and reordering grammar
  instructor: "Miloš Stanojević"
  note: 
  abstract: In this lecture we will talk about preordering, factorization of permutations and reordering grammar.
  background:
    - "Stanojevic ́ and Sima'an. 2015. [Reordering Grammar Induction](http://www.aclweb.org/anthology/D15-1005)"
  discussion:
    - "Tromble and Eisner. 2009. [Learning Linear Ordering Problems for Better Translation](http://www.aclweb.org/anthology/D09-1105)"
  slides: resources/slides/rg.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-04-26
  img: labelling
  uid: labelling
  title: Labelling Hiero
  instructor: Gideon Wenniger
  note: 
  abstract: |
    In this session we will see how labels can be added
    too Hiero to improve word order and lexical choice.
    We will motivate why to label Hiero grammars, and
    discuss what types of label have been considered
    in the literature.

    As a first example we will look at a well known labeling
    approach for the target side known as syntax-augmented
    machine translation (SAMT).
    SAMT uses syntactic information from the target side.

    We will then discuss how labeling can be done using
    only the information about hierarchical translation
    equivalence relations induced by word alignments.
    This information describes the type of reordering taking 
    place within translation rules more directly than syntax.

    The lecture will be ended with a discussion of
    applying labels as soft constraints (fuzzy matching)
    in labeling approaches, and why this is important
    for success.

    Summary 
    In this session students will get a better understanding of:

    1. What rule nonterminal labeling is and why it is done
    2. What types of labels can be constructed, using syntax, dependency and other 
        information or just the information present in word alignments.
    3. What the important differences are between source and target side labeling,
        both in motivation as well as in technical consequences for implementation 
        and effects on sparsity etc.
    4. What soft matching approaches for labeling are and why they are important.
  background:
    - "Zollmann and Venugopal. 2006. [Syntax Augmented Machine Translation via Chart Parsing](http://www.aclweb.org/anthology/W/W06/W06-3119.pdf)"
    - "Wenniger and Sima'an. 2014. [Bilingual Markov Reordering Labels for Hierarchical SMT](https://www.aclweb.org/anthology/W/W14/W14-4002.pdf)"
  further: 
    - "[Mylonakis and Sima'an (2010)](http://www.aclweb.org/anthology/W10-2915) describe how distributions over labels similar to SAMT labels can be learned from data using a variant of the EM algorithm known as cross-validating EM. This paper uses a combination of a labeled version of ITG in combination with phrase pairs. Labels are applied on the source side, similar to the way this is done in ``Using Syntactic Head Information in Hierarchical Phrase-Based Translation'' but with two main differences: 1. The label distributions are learned rather than heuristically estimated. 2. The label matching constraint is enforced exactly inside the (adapted) decoder, instead of approximately by using grammar filtering."
  discussion:
    - "[Chiang (2010)](http://www.aclweb.org/anthology/P10-1146) explains how source and target syntax can be effectively combined for improved translation. The key to the success of the proposed approach is matching the labels in a soft rather than strict way, using dedicated features that indicate substitution of label X to label Y. Another interesting thing is that there is only one labeled rule version per Hiero rule, so that the size of the grammar remains equal to unlabeled Hiero. This is significant, because for SAMT in contrast, labeling introduces many alternatively labeled variants of the same Hiero, increasing the size of the grammar drastically. Finally the paper introduces an interesting scheme to further reduce the size of the grammar, and increase decoding speed, without hampering performance."
    - "[Li et al. (2012)](http://www.aclweb.org/anthology/W/W12/W12-3128.pdf) describe how dependency parse information can be exploited to construct effective source labels for Hiero rules. Source labeling has a different motivation than target labeling. Target labels can be thought of as working mainly as a kind of language models. Source labels in contrast typically are used to provide a stronger context for the application of rules, by enforcing that only rules matching the source parse chart information (in this case dependency parse information) can be used by the decoder. In this paper the latter is effectively only approximated, by filtering the grammar to contain rules with right-hand-sides matching labels for any of the source side labels of sentences in the development or testing set. Note that a more precise implementation involves adapting the decoder to include a source label chart along with the input and enforce the matching with this label chart inside the decoder."
  slides: resources/slides/labelling.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-04-28
  img: morpho
  uid: morpho
  title: Morphology in machine translation
  instructor: Joachim Daiber
  note: 
  abstract: "In this lecture we will discover that not all language pairs are created equal. We will discuss some problems that morphologically rich languages pose and how they are approached in machine translation."
  background: 
    - "Soricut and Och. 2015. [Unsupervised Morphology Induction Using Word Embeddings](http://www.aclweb.org/anthology/N15-1186.pdf)"
    - "Sennrich and Haddow. 2015. [A Joint Dependency Model of Morphological and Syntactic Structure for Statistical Machine Translation.](http://www.aclweb.org/anthology/D/D15/D15-1248.pdf)"
  discussion:
    - "Koehn and Hoang. 2007. [Factored Translation Models](http://homepages.inf.ed.ac.uk/pkoehn/publications/emnlp2007-factored.pdf)"
    - "Williams and Koehn. 2011. [Agreement Constraints for Statistical Machine Translation into German](http://www.aclweb.org/anthology/W11-2126.pdf)"
  further:
    - "Compounds have long been a problem for word- and phrase-level machine translation. Soricut and Och's method for morphology can be extended to split compounds into their meaningful parts."
    - "Daiber et al. 2015. [Splitting Compounds by Semantic Analogy](http://jodaiber.github.io/doc/compound_analogy.pdf)"
  slides: resources/slides/morphology.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-05-03
  img: nmt
  uid: nmt
  title: Neural models for translation
  instructor: Joost Bastings
  note: 
  abstract: >
    In the last couple of years, deep learning has proven useful in many areas. 
    Also for machine translation, models have been proposed, in some cases reaching state-of-the-art performance. 
    In this lecture we will look at the (now classic) encoder-decoder framework, as well as more recent models.
  background:
    - "Sutskever et al. 2014. [Sequence to Sequence Learning with Neural Networks](http://arxiv.org/abs/1409.3215)"
    - "Cho et al. 2014. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/abs/1406.1078)"
  further:
    - One problem with the models of the background literature is that they use a single vector to represent the source sentence. [Attention models](http://arxiv.org/abs/1409.0473) address this limitation.
  discussion:
    - "Jean et al. 2015. [On Using Very Large Target Vocabulary for Neural Machine Translation](http://www.aclweb.org/anthology/P15-1001.pdf)"
    - "Chung et al. 2016. [A Character-level Decoder without Explicit Segmentation for Neural Machine Translation](http://arxiv.org/abs/1603.06147)"
    - "Luong et al. 2015. [Effective Approaches to Attention-based Neural Machine Translation](http://arxiv.org/abs/1508.04025)"
  slides: resources/slides/nmt.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-05-10
  img: multi
  uid: multimt
  title: Multimodal MT
  instructor: Desmond Elliot
  note: 
  abstract:  In this lecture we will discuss approaches to translating sentences that occur alongside images. In this context, the images can be seen as a common grounding for both languages, potentially easing the translation task.
  background:
      - It will be useful to attend the Neural models for translation lecture.
      - "Neural sequence models for multimodal machine translation: [Elliott et al. (2015)](http://arxiv.org/abs/1510.04709)"
      - "Multimodal target-side re-ranking [Hitschler et al. (2016)](http://arxiv.org/abs/1601.03916)"
  discussion:
      - "[Are You Talking to a Machine?](http://arxiv.org/abs/1505.05612) Dataset and Methods for Multilingual Image Question Answering"
  slides: resources/slides/multimodal.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2016-05-12
  img: transfer
  uid: transfer
  title: Parallel texts as a linguistic resource
  instructor: Stella Frank
  note: 
  abstract: >
    In this lecture we will look at using parallel and multiply-aligned text for non-machine translation tasks, such as  part-of-speech tagging and parsing. The motivation behind this line of work is the idea that good models (parsers, taggers) for high-resource languages can be transfered to low-resource languages, which do not have sufficient (annotated) data to train good models on their own. Secondly, in an unsupervised setting, multilingual data can provide cues that disambiguate across languages.
  background:
    - "Snyder et al. 2008[Unsupervised POS tagging over two languages at once is easier/better than monolingual models](http://www.aclweb.org/anthology/D/D08/D08-1109.pdf)."
    - "McDonald et al. 2011. [How to apply a trained parser from language A to language B](http://www.aclweb.org/anthology/D/D11/D11-1006.pdf)"
  further:
    - "'Universal' (at least standardised) annotation [guidelines and treebanks](http://universaldependencies.org/) for multiple languages."
    - "Non-parametric [follow-up](http://www.aclweb.org/anthology/N/N09/N09-1010.pdf) to (Snyder et al., 2008)"
    - "[Follow-up](http://www.aclweb.org/anthology/N/N13/N13-1126.pdf) to (McDonald et al, 2011)."
    - "Quasi-synchronous grammars for annotation projection: [foundation](http://www.aclweb.org/anthology/W/W06/W06-3104.pdf), [parser adaptation](http://www.aclweb.org/anthology/D/D09/D09-1086.pdf)"
  discussionwarn: On multilingual representations in neural models
  discussion:
    - "Hermann and Blunsom. 2014. [Multilingual Models for Compositional Distributed Semantics](http://www.aclweb.org/anthology/P/P14/P14-1006.pdf)"
    - "Ammar et al. 2016. [Massively Multilingual Word Embeddings](http://arxiv.org/abs/1602.01925)"
  slides: resources/slides/transfer.pdf
  code: 
  data: 

