-
  layout: lecture
  selected: y
  date: 2017-05-12
  img: nmt
  title: Language models
  instructor: Joost Bastings
  note: 
  abstract: 
  background:
  further:
  discussion:
  slides: 
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2017-05-17
  img: nmt
  title: "Conditional language models: encoder-decoder"
  instructor: Wilker Aziz
  note: 
  abstract: >
    In the last couple of years, deep learning has proven useful in many areas. 
    Also for machine translation, models have been proposed, in some cases reaching state-of-the-art performance. 
    In this lecture we will look at the (now classic) encoder-decoder framework, as well as more recent models.
  background:
    - "Sutskever et al. 2014. [Sequence to Sequence Learning with Neural Networks](http://arxiv.org/abs/1409.3215)"
    - "Cho et al. 2014. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/abs/1406.1078)"
  further:
    - One problem with the models of the background literature is that they use a single vector to represent the source sentence. [Attention models](http://arxiv.org/abs/1409.0473) address this limitation.
  discussion:
    - "Jean et al. 2015. [On Using Very Large Target Vocabulary for Neural Machine Translation](http://www.aclweb.org/anthology/P15-1001.pdf)"
    - "Chung et al. 2016. [A Character-level Decoder without Explicit Segmentation for Neural Machine Translation](http://arxiv.org/abs/1603.06147)"
    - "Luong et al. 2015. [Effective Approaches to Attention-based Neural Machine Translation](http://arxiv.org/abs/1508.04025)"
  slides: resources/slides/nmt.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2017-05-19
  img: nmt
  title: Convolutional encoders
  instructor: Joost Bastings
  note: 
  abstract:  
  background:
  discussion:
  slides: 
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2017-05-24
  img: nmt
  title: "Align-Embed: variational auto-encoders"
  instructor: Wilker Aziz
  note: 
  abstract:  
  background:
  discussion:
  slides: 
  code: 
  data: 
