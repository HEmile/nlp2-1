-
  layout: lecture
  selected: y
  date: 2017-04-21
  img: pbsmt
  title: "Linear models: phrase-based and hierarchical phrase-based SMT"
  instructor: Miguel Rios
  note: 
  abstract: |
    In this lecture we will introduce a phrase-based model and its hierarchical extension. 
    We will motivate the need for larger translation units than word pairs, and show how their modelling introduces a new issue: phrase segmentation. 
    We will see how standard (hierarchical) phrase-based SMT deals with this issue, and the model's parameterisation.
  background:
    - "Koehn et al. 2003. [Statistial Phrase-Based Translation](http://www.aclweb.org/anthology/N/N03/N03-1017.pdf)"
    - "[Kumar and Byrne (2003)](http://www.aclweb.org/anthology/N03-1019) present a derivation and implementation of the [alignment template model](http://www.aclweb.org/anthology/W99-0604) using FSTs."
    - "David Chiang. [An introduction to synchronous grammars](resources/papers/ChiangSCFG.pdf)"
    - "Chiang. 2007. [Hierarchical Phrase-Based Translation](http://www.mitpressjournals.org/doi/pdf/10.1162/coli.2007.33.2.201)"
    - "Chiang. 2005. [A Hierarchical Phrase-Based Model for Statistical Machine Translation](http://www.aclweb.org/anthology/P05-1033)" 
  further:
    - "On estimation:"
    - "[Marcu and Wong (2002)](http://www.aclweb.org/anthology/W02-1018) proposed a model that is conceptually very similar to the IBM models 1 and 2, but applied to phrases instead of words. Estimation in that model is intractable, requiring initial approximations and hard EM."
    - "[DeNero et al. (2006)](http://www.aclweb.org/anthology/W06-3105) inferred phrase translation estimates in a (uniform) segmentation space constrained by word alignments trained with the IBM models. This leads to degenerate estimates."
    - "[Mylonakis and Sima'an (2008)](http://www.aclweb.org/anthology/D08-1066) used word alignments and the phrase-extraction method of [Koehn et al. (2003)](http://www.aclweb.org/anthology/N/N03/N03-1017.pdf), but infer phrase-translation estimates with EM. They restrict the segmentation space to ITG derivations, and use cross-validation techniques to avoid overfitting."
    - "[DeNero and Klein (2008)](http://www.aclweb.org/anthology/P08-2007) showed that estimation for phrase-based models is NP-complete." 
    - "On inference:"
    - "[Sima'an (2002)](https://staff.fnwi.uva.nl/k.simaan/D-Papers/Grammars01.pdf) proved that disambiguation of formal grammars (Stochastic Context-Free and Tree-Substitution Grammars) is NP-complete. The result carries over to any problem where one needs to sum over derivations (or segmentations in the case of this lecture)."  
    - "[Knight (1999)](http://www.aclweb.org/anthology/J/J99/J99-4005.pdf) proved that decoding (for word-based models) is NP-complete, and laid out the analogy between decoding and the Travelling Salesman Problem (TSP)."
    - "[Zaslavskiy et al. (2009)](http://www.aclweb.org/anthology/P/P09/P09-1038.pdf) proposed to apply TSP solvers for decoding instead of beam-search decoding."
    - "[Aziz et al. (2014)](http://www.aclweb.org/anthology/D14-1131) replace beam-search decoding by an exact coarse-to-fine search inspired by rejection sampling."
  classmaterial: 
    - "Part 1: [PBSMT](resources/slides/pbsmt.pdf)"
    - "Part 2: [HPBSMT](resources/slides/hpbsmt.pdf)"
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2017-04-26
  img: tuning
  title: Evaluation and Tuning
  instructor: "Miloš Stanojević"
  note: 
  abstract: >
    In the first half of this lecture we will talk about automatic evaluation. 
    Evaluation in Machine Translation is dealing with automatic assesment of the translation quality. 
    Unlike many machine learning tasks, here there is no unique gold truth to which we can compare the translation -- there are many equally good translations. 
    Also translations can be good or bad in many dimensions: morphology, syntax or semantics. 
    Evaluation is important not only for assesing the quality of the final system but also for automatic tuning of the systems. 

    In the second half, we will cover a few algorithms that allow us to fine tune many parameters that our MT system can have (weights for language model, translation model, word penalty...).
  background:
    - "On evaluation:"
    - "A good survey of evaluation methods can be found in Chapter 8 of [Koehn's book](http://www.statmt.org/book/). You can also refer to statmt's survey [page](http://www.statmt.org/survey/Topic/Evaluation)"
    - "[BLEU](http://www.aclweb.org/anthology/P02-1040.pdf) is the most widely used automatic evaluation metric, it is mostly a precision metric."
    - "On tuning:"
    - "A [survey](http://www.phontron.com/paper/neubig16cl.pdf) of optimisation techniques for machine translation."
    - "MERT is nicely explained in [Koehn's book](http://www.statmt.org/book/) (Chapter 9.3.2: Powell search algorithm)"
    - "MT tuning can be seen as a learning to rank task. In learning to rank, a typical approach is to approximate a complex ranking task by a series of pairwise ranking decision. This idea has been popularised in MT by the algorithm [PRO](http://www.aclweb.org/anthology/D11-1125)."
    - "For a nice overview of large margin discriminative learning in MT refer to [(Eidelman, 2012)](http://www.aclweb.org/anthology/W12-3160)"
  further:
    - "On evaluation:"
    - "[BEER](http://aclweb.org/anthology/D14-1025) is a trainable metric meant for approximating human ranking of translations."
    - "[METEOR](http://www.aclweb.org/anthology/W/W05/W05-0909.pdf) tries to balance precision and recall."
    - "Read [(Lopez, 2012)](http://www.aclweb.org/anthology/W/W12/W12-3101.pdf) for a formal view of evaluation of automatic evaluation metrics (you read it right!)." 
    - "[Hypothesis testing for SMT](http://www.aclweb.org/anthology/P/P11/P11-2031.pdf) or whether two systems differ."
    - "On tuning:"
    - "For MIRA with batched updates in MT refer to [(Cherry and Forster, 2012)](http://www.aclweb.org/anthology/N12-1047)"
  discussion:
  slides: resources/slides/mteval.pdf
  classmaterial: 
    - "Part 1: [Evaluation](resources/slides/mteval.pdf)"
    - "Part 2: [Tuning](resources/slides/tuning.pdf)"
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2017-05-03
  img: rg
  title: Factorisation of permutations and reordering grammar
  instructor: "Miloš Stanojević"
  note: 
  abstract: In this lecture we will talk about preordering, factorization of permutations and reordering grammar.
  background:
    - "Stanojevic ́ and Sima'an. 2015. [Reordering Grammar Induction](http://www.aclweb.org/anthology/D15-1005)"
  discussion:
    - "Tromble and Eisner. 2009. [Learning Linear Ordering Problems for Better Translation](http://www.aclweb.org/anthology/D09-1105)"
  slides: resources/slides/rg.pdf
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2017-05-10
  img: itg
  title: "Probablistic models: ITG and LVM-CRF"
  instructor: Wilker Aziz
  note: 
  abstract: In this lecture we will present probabilistic models built on top of synchronous context-free grammars.
  background:
    - "ITG is a form of context-free transducer. Thus the more you know about context-free grammars (CFGs) and their probabilistic extensions (PCFGs), the better. You can start with M Collins's [lecture notes](http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf) on PCFGs. In estimating parameters for a PCFG with EM we need expectations (as always), there is an efficient algorithm to compute those from parse forests. Again, M Collins's has nice [lecture notes](http://www.cs.columbia.edu/~mcollins/io.pdf) on this algorithm (the Inside-Outside algorithm) as well as EM for PCFGs."
    - "[Wu (1997)](https://www.aclweb.org/anthology/J/J97/J97-3002.pdf) is the best introduction to ITG in the literature."
    - "IBM alignment models fit within a larger translation component inspired by the noisy channel approach from information theory. ITGs can also play that role, see [Wu and Wong (1998)](http://www.aclweb.org/anthology/P98-2230). "
  further:
    - "ITGs have a very generic vocabulary of nonterminals leading to very flat distributions. One can use the idea of state splitting to refine the vocabulary of a vanilla ITG. This is very difficul to do in terms of MLE. [Blunsom et al. (2008)](http://papers.nips.cc/paper/3453-bayesian-synchronous-grammar-induction) develop a non-parametric Bayesian model which learns how far to refine the vocabulary of a vanilla ITG. [Blunsom and Cohn (2010)](http://www.aclweb.org/anthology/N10-1028) build on that work and propose a more efficient inference technique based on slice sampling."
    - "You probably want to wait until after the lecture on 'Phrase-based models' before you read the following papers."
    - "Phrase-based ITGs have never shown state-of-the-art performance because of its degenerate maximum likelihood estimates. [Mylonakis and Sima'an (2010)](http://www.aclweb.org/anthology/W10-2915) propose a better estimation procedure."
    - "ITGs impose hard constraints on reordering, yet these constraints are not as unmotivated as an ad-hoc distortion limit. [Zens et al. (2004)](http://www.aclweb.org/anthology/C04-1030) investigate the effectiveness of ITG-style constraints in a phrase-based decoder. They also compare it to IBM-style constraints (namely, distortion limit)." 
  discussion:
  slides: resources/slides/itg.pdf
  code: 
  data: 
