---
layout: post
title:  Project 2 
date:   2017-05-03
author: Wilker
categories: projects
---

[Project 2 description]({{ "/resources/project_crf/project2.pdf" | absolute_url }}).

Deadline: May 19 at 23:59 GMT-8.


# Code

* [roadmap](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/LV-CRF-Roadmap.ipynb): a tutorial style description of the project that connects theory and practice; in particular, we connect the concepts discussed in class with actual code; this tutorial uses `libitg` (see below);
* [libitg](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/libitg.py): is our implementation of Earley intersection between epsilon-free unweighted FSA and CFGs, we also provide functions to deal with ITGs and length constraints;
* [notebook on intersection](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/ITG.ipynb): you can use this notebook to understand more about the bits and pieces of the parser, but do not copy code from there, for that use [libitg](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/libitg.py)

# Reading

* Model
    * [Lecture slides]({{ "/resources/slides/crf.pdf" | absolute_url }})
    * [LV-CRF for Alignment](http://www.aclweb.org/anthology/P11-1042)
    * [LV-CRF for SMT](http://www.aclweb.org/anthology/P08-1024)
    * [Supervised CRFs](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)
    * [LV-CRF](https://www.cics.umass.edu/~mccallum/papers/entropygradient-naacl2007.pdf)
* Synchronous grammars
    * [ITGs](https://www.aclweb.org/anthology/J/J97/J97-3002.pdf)
    * [SCFG tutorial by David Chiang]({{ "/resources/papers/ChiangSCFG.pdf" | absolute_url }})
    * [Lecture slides]({{ "/resources/slides/itg.pdf" | absolute_url }})
* Bitext parsing
    * [Lecture slides]({{ "/resources/slides/bitext-parsing.pdf" | absolute_url }})
    * [Notes on Earley intersection]({{ "/resources/papers/Aziz-Earley.pdf" | absolute_url }})
    * [Principles and implementation of deductive parsing](https://arxiv.org/abs/cmp-lg/9404008)
    * [Cascade of parsers](http://www.aclweb.org/anthology/N10-1033)
* Hypergraphs
    * [Directed hypergraphs]({{ "/resources/papers/GalloEtAl.pdf" | absolute_url }})
    * [Parsing and hypergraphs](https://nlp.stanford.edu/manning/papers/klein_and_manning-parsing_and_hypergraphs-IWPT_2001.pdf)
* FSAs
    * [Knight and May's chapter on applications of weighted automata to NLP]({{ "/resources/papers/KnightAndMay.pdf" | absolute_url }})
    * [FSTs in language and speech processing](http://www.cs.nyu.edu/~mohri/pub/cl1.pdf)
    * [Mohri's chapter on weighted automata algorithms]({{ "/resources/papers/MohriWAA.pdf" | absolute_url }})
* Semirings 
    * [Goodman's semiring parsing artcile](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf) 
    * [Expectation semiring for translation forests](http://www.aclweb.org/anthology/D09-1005)

# Tips

* To construct an ITG use the template in section 1 of the project description and the lexicon of translation pairs that we provided.
* ITGs can be rather large and the resulting forests may be quite nasty to deal with, thus we recommend you constrain your ITG so that it contains only top-scoring translation pairs (you can use the IBM1 probabilities that we provided, you can choose one direction or interpolate both). For development we recommend you work with at most 5 translations per source word, once everything is working, you can investigate the impact of having more options.
* Note that your ITG has to be able to insert and delete words, thus it has to have translation pairs involving the empty string, for that you can use alignments to NULL in the lexicon we provided.
* You can pre-compute parse forests and feature functions and save them to disk (for example using `cPickle`), that should save some time during iterations of SGD.


# Unknown types

Note that a proper ITG will always cover the training data. That's so because the lexicon would map every source type to every target type,  allow every source type to be deleted, and every target type to be inserted. Because we do not want to overoptimise our implementations, we will be working with a constrained lexicon, this means that sometimes you will not be able to parse the source and/or the target observation. This may lead to an empty `D(x)` and/or an empty `D(x,y)` which makes the training instance useless.

The simplest strategy is to discard training instances for which D(x) and/or D(x,y) are empty, but that's a bit unsatisfactory and one may end up discarding too much data. Also, it is not general enough a solution and the problem will happen again with dev/test sets.

A better solution is to deal with it! One approach is to have a base lexicon (the one with 5 translation options per Chinese type) with some sentence-specific modifications. For example, make sure that before you parse a Chinese sentence, every word in that sentence gets the chance to be deleted. That is, add rules `X -> x/eps` where `x` is a type in the Chinese sentence. If you are training (thus parsing string pairs), you can make sure that every target word gets a chance to be inserted. That is, add rules `X -> eps/y` where `y` is a type in the English sentence. Note that if the added rule is not really support by your base lexicon, it's wise to flag it as such with a feature (e.g. `type:out-of-source-lexicon`, `type:out-of-target-lexicon`).

Deletions are never too costly, but insertions can be quite expensive. If that becomes an issue, an alternative is to have an ITG rule that maps unseen types to unseen types (e.g. `X -> unk/unk`) and pre-process the training set replacing types that are not in your constrained lexicon with `unk` on either side. Note that you can do the same with respect to epsilon rules (e.g. `X -> unk/eps` and `X -> eps/unk`). These solutions are roughly equivalent, but one may be slower to parse than the other. It's entirely up to you to decide which strategy you want to work with. You can also come up with alternatives. In the end of the day, simply report what you did and try to justify it. 

# More features

Because you have access to the source sentence at all times, you can get rich source features of the source spans and edge covers. These features may help you get better segmentation and word order.

I call inside/outside span features those that summarise the context under a certain span and surrounding a certain span. For example, for an edge `X[2-6] -> X[2-3] X[3-6]` we know exactly which source phrase stands under 2-6, 2-3 and 3-6, in fact we also know which phrases stand in [bos-2] and [6-eos]. Phrases are very sparse though, thus they would make our feature vectors really huge! A good idea might be to use word embeddings to represent words (as a preprocessing step) and have an average embedding represent a phrase (in this case you would be adding as many features as dimensions in your embedding vector). You can also go beyond and use LSTM representations (if you know how to get them) which more naturally represent spans of text.

You can also use source features for reordering, for example, skip bigrams. A skip-bigram is a bigram which does not have to be adjacent, for example, in `le chien noir`, `le * chien`, `le * noir`, `chien * noir` are skip-bigrams. Because you always know the source span and the source string, you can always retrieve skip-bigrams. 

# Target language model features

Target language model features are really hard to use! I can help you with that in the next lecture if you are curious, but that's not going to be expected for project 2 as I would have to teach you a lot more about smart approximate intersection algorithms ;)
I am very familiar with those though, so if you are interested to go the extra mile, talk to me.
