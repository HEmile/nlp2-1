---
layout: post
title:  Project 2 
date:   2017-05-03
author: Wilker
categories: projects
---

[Project 2 description]({{ "/resources/project_crf/project2.pdf" | absolute_url }}).

Deadline: May 19 at 23:59 GMT-8.


# Code

* [roadmap](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/LV-CRF-Roadmap.ipynb): a tutorial style description of the project that connects theory and practice; in particular, we connect the concepts discussed in class with actual code; this tutorial uses `libitg` (see below);
* [libitg](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/libitg.py): is our implementation of Earley intersection between epsilon-free unweighted FSA and CFGs, we also provide functions to deal with ITGs and length constraints;
* [notebook on intersection](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/ITG.ipynb): you can use this notebook to understand more about the bits and pieces of the parser, but do not copy code from there, for that use [libitg](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/libitg.py)

# Reading

* Model
    * [Lecture slides]({{ "/resources/slides/crf.pdf" | absolute_url }})
    * [LV-CRF for Alignment](http://www.aclweb.org/anthology/P11-1042)
    * [LV-CRF for SMT](http://www.aclweb.org/anthology/P08-1024)
    * [Supervised CRFs](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)
    * [LV-CRF](https://www.cics.umass.edu/~mccallum/papers/entropygradient-naacl2007.pdf)
* Synchronous grammars
    * [ITGs](https://www.aclweb.org/anthology/J/J97/J97-3002.pdf)
    * [SCFG tutorial by David Chiang]({{ "/resources/papers/ChiangSCFG.pdf" | absolute_url }})
    * [Lecture slides]({{ "/resources/slides/itg.pdf" | absolute_url }})
* Bitext parsing
    * [Lecture slides]({{ "/resources/slides/bitext-parsing.pdf" | absolute_url }})
    * [Notes on Earley intersection]({{ "/resources/papers/Aziz-Earley.pdf" | absolute_url }})
    * [Principles and implementation of deductive parsing](https://arxiv.org/abs/cmp-lg/9404008)
    * [Cascade of parsers](http://www.aclweb.org/anthology/N10-1033)
* Hypergraphs
    * [Directed hypergraphs]({{ "/resources/papers/GalloEtAl.pdf" | absolute_url }})
    * [Parsing and hypergraphs](https://nlp.stanford.edu/manning/papers/klein_and_manning-parsing_and_hypergraphs-IWPT_2001.pdf)
* FSAs
    * [Knight and May's chapter on applications of weighted automata to NLP]({{ "/resources/papers/KnightAndMay.pdf" | absolute_url }})
    * [FSTs in language and speech processing](http://www.cs.nyu.edu/~mohri/pub/cl1.pdf)
    * [Mohri's chapter on weighted automata algorithms]({{ "/resources/papers/MohriWAA.pdf" | absolute_url }})
* Semirings 
    * [Goodman's semiring parsing artcile](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf) 
    * [Expectation semiring for translation forests](http://www.aclweb.org/anthology/D09-1005)

# Tips

* To construct an ITG use the template in section 1 of the project description and the lexicon of translation pairs that we provided.
* ITGs can be rather large and the resulting forests may be quite nasty to deal with, thus we recommend you constrain your ITG so that it contains only top-scoring translation pairs (you can use the IBM1 probabilities that we provided, you can choose one direction or interpolate both). For development we recommend you work with at most 5 translations per source word, once everything is working, you can investigate the impact of having more options.
* Note that your ITG has to be able to insert and delete words, thus it has to have translation pairs involving the empty string, for that you can use alignments to NULL in the lexicon we provided.
* You can pre-compute parse forests and feature functions and save them to disk (for example using `cPickle`), that should save some time during iterations of SGD.


# Unknown types

Note that a proper ITG will always cover the training data. That's so because the lexicon would map every source type to every target type,  allow every source type to be deleted, and every target type to be inserted. Because we do not want to overoptimise our implementations, we will be working with a constrained lexicon, this means that sometimes you will not be able to parse the source and/or the target observation. This may lead to an empty `D(x)` and/or an empty `D(x,y)` which makes the training instance useless.

The simplest strategy is to discard training instances for which D(x) and/or D(x,y) are empty, but that's a bit unsatisfactory and one may end up discarding too much data. Also, it is not general enough a solution and the problem will happen again with dev/test sets.

A better solution is to deal with it! One approach is to have a base lexicon (the one with 5 translation options per Chinese type) with some sentence-specific modifications. For example, make sure that before you parse a Chinese sentence, every word in that sentence gets the chance to be deleted. That is, add rules `X -> x/eps` where `x` is a type in the Chinese sentence. If you are training (thus parsing string pairs), you can make sure that every target word gets a chance to be inserted. That is, add rules `X -> eps/y` where `y` is a type in the English sentence. Note that if the added rule is not really support by your base lexicon, it's wise to flag it as such with a feature (e.g. `type:out-of-source-lexicon`, `type:out-of-target-lexicon`).

Deletions are never too costly, but insertions can be quite expensive. If that becomes an issue, an alternative is to have an ITG rule that maps unseen types to unseen types (e.g. `X -> unk/unk`) and pre-process the training set replacing types that are not in your constrained lexicon with `unk` on either side. Note that you can do the same with respect to epsilon rules (e.g. `X -> unk/eps` and `X -> eps/unk`). These solutions are roughly equivalent, but one may be slower to parse than the other. It's entirely up to you to decide which strategy you want to work with. You can also come up with alternatives. In the end of the day, simply report what you did and try to justify it. 

