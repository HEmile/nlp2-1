---
layout: default
title: Projects
---

*Spring 2017*

We want you to get your hands dirty with most of the core topics covered in the course. 
To that end, we are preparing an interesting project with three milestones. 
See below for description and deadlines.

# Project 1: word alignment 

*From March 12 to March 26*

In this project you will learn about latent variable modelling with directed graphical models (aka locally normalised models).
The unsupervised problem is word alignment. The model is parameterised by categorical distributions. 
You will experiment with maximum likelihood estimation and Bayesian modelling with Dirichlet priors.

* Maximum likelihood estimation for IBM1: EM algorithm
* Bayesian estimation for IBM1: variational Bayes

# Project 2: reordering model 

*From March 26 to May 17*

In this project you will learn about a different class of probabilistic graphical models, namely, undirected models (aka globally normalised models). 
Undirected graphical models are much harder to learn, thus we will focus on a supervised task. We will focus on the problem of learning how to permute a sequence of words into target language word order, we will frame this truly unsupervised learning problem as a supervised one by relying on word alignments as a source of observations. We will use pre-trained alignments to extract a cannonical tree-structured mapping between the source word order and the target word order. 

* Maximum likelihood estimation for a CRF parser: CKY algorithm, inside-outside algorithm, and gradient-based optimisation

# Project 3:  

*From May 17 to June 7*

In this project you will learn about maximum likelihood estimation for graphical models parameterised by neural networks.

Details to be announced.

